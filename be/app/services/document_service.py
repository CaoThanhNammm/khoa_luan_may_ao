import ast
import asyncio
import json
import re
import time

from botocore.exceptions import NoCredentialsError, ClientError
from sqlalchemy.orm import Session
from app.models.document import UserDocument, ContactMessage
from app.schemas.document import DocumentCreate, ContactMessageCreate
from app.services.user_service import get_user_by_id
from fastapi import HTTPException, status, UploadFile
import os
import uuid
from dotenv import load_dotenv
from typing import Optional
from app.core.global_instances import (
    get_pdf, get_llama_chunks, get_llama_title, get_llama_content,
    get_qdrant, get_neo, get_preprocessing, get_s3_client
)
from LLM.prompt import extract_entities_relationship_from_text, chunking, create_title
import ast
import traceback
from fastapi import HTTPException
load_dotenv()

# AWS S3 Configuration
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME", "khoaluan111")
S3_REGION = os.getenv("S3_REGION", "ap-southeast-1")

# Global instances will be initialized lazily when needed
def _get_global_instances():
    """Get all global instances lazily"""
    return {
        'pdf': get_pdf(),
        'llama_chunks': get_llama_chunks(),
        'llama_title': get_llama_title(),
        'llama_content': get_llama_content(),
        'qdrant': get_qdrant(),
        'neo': get_neo(),
        'pre_processing': get_preprocessing(),
        's3_client': get_s3_client()
    }

def get_user_documents(db: Session, user_id: int):
    """Get all documents for a user"""
    return db.query(UserDocument).filter(UserDocument.user_id == user_id).all()

def get_document(db: Session, document_id: int):
    """Get a document by ID"""
    return db.query(UserDocument).filter(UserDocument.id == document_id).first()

def get_document_by_external_id(db: Session, document_id: str, user_id: int):
    """Get a document by external ID (document_id field)"""
    return db.query(UserDocument).filter(
        UserDocument.document_id == document_id,
        UserDocument.user_id == user_id
    ).first()

def create_document(db: Session, document: DocumentCreate, user_id: int):
    """Create a new document record"""
    user = get_user_by_id(db, user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    db_document = UserDocument(
        user_id=user_id,
        conversation_id=document.conversation_id,
        document_id=document.document_id,
        filename=document.filename,
        file_size=document.file_size,
        status=document.status,
        s3_key=document.s3_key,
        s3_url=document.s3_url
    )
    
    db.add(db_document)
    db.commit()
    db.refresh(db_document)
    
    return db_document

def update_document_status(db: Session, document_id: str, status: str, user_id: int):
    """Update a document's status"""
    document = get_document_by_external_id(db, document_id, user_id)
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Document not found"
        )
    
    document.status = status
    db.commit()
    db.refresh(document)
    
    return document

def upload_to_s3(file_content: bytes, file_name: str, document_id: str) -> dict:
    """Upload a file to S3"""
    instances = _get_global_instances()
    s3_client = instances['s3_client']
    
    if not s3_client:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="S3 client not initialized"
        )
    
    try:
        # Create S3 key with document_id to avoid duplicates
        s3_key = f"documents/{document_id}/{file_name}"
        
        # Upload file to S3
        s3_client.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_key,
            Body=file_content,
            ContentType='application/pdf'
        )
        
        s3_url = f"s3://{S3_BUCKET_NAME}/{s3_key}"
        
        return {
            "s3_key": s3_key,
            "s3_url": s3_url
        }
    except Exception as e:
        print(f"Error uploading to S3: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to upload to S3: {str(e)}"
        )

async def process_file_upload(file: UploadFile, user_id: int, db: Session):
    """Process a file upload"""
    # Validate file
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No file uploaded"
        )
    
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Only PDF files are accepted"
        )
    
    # Generate document ID
    document_id = str(uuid.uuid4())
    
    # Read file content
    contents = await file.read()
    if not contents:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="File is empty or could not be read"
        )
    
    # Upload to S3
    s3_info = upload_to_s3(contents, file.filename, document_id)
    print('in main: ' + document_id)
    # Create document record
    document = DocumentCreate(
        document_id=document_id,
        filename=file.filename,
        file_size=len(contents),
        status="processing",
        s3_key=s3_info["s3_key"],
        s3_url=s3_info["s3_url"]
    )
    
    db_document = create_document(db, document, user_id)

    # Process PDF after uploading to S3
    try:
        # Add timeout for PDF processing (20 minutes)
        await asyncio.wait_for(
            process_pdf(s3_info["s3_url"], s3_info["s3_key"], document_id),
            timeout=1200  # 20 minutes = 1200 seconds
        )
        # Update document status to completed
        update_document_status(db, document_id, "completed", user_id)
        final_status = "completed"
        print(f"üéâ PDF processing completed for document: {document_id}")
    except asyncio.TimeoutError:
        print(f"‚è∞ PDF processing timeout for document: {document_id}")
        update_document_status(db, document_id, "failed", user_id)
        final_status = "failed"
    except Exception as e:
        print(f"‚ùå Error processing PDF for document {document_id}: {str(e)}")
        # Update document status to failed
        update_document_status(db, document_id, "failed", user_id)
        final_status = "failed"

    return {
        "document_id": document_id,
        "filename": file.filename,
        "file_size": len(contents),
        "s3_key": s3_info["s3_key"],
        "s3_url": s3_info["s3_url"],
        "status": final_status
    }



def upload_to_s3_key(file_content: bytes, file_name: str, document_id: str) -> str:
    """
    Upload file to S3 and return the S3 key
    """
    try:
        instances = _get_global_instances()
        s3_client = instances['s3_client']
        
        if not s3_client:
            raise Exception("S3 client not initialized")

        # T·∫°o S3 key v·ªõi document_id ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        s3_key = f"documents/{document_id}/{file_name}"

        # Upload file to S3
        s3_client.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_key,
            Body=file_content,
            ContentType='application/pdf'
        )

        print(f"File uploaded to S3: s3://{S3_BUCKET_NAME}/{s3_key}")
        return s3_key

    except NoCredentialsError:
        print("AWS credentials not found")
        raise Exception("AWS credentials not configured")
    except ClientError as e:
        print(f"AWS S3 error: {str(e)}")
        raise Exception(f"Failed to upload to S3: {str(e)}")
    except Exception as e:
        print(f"Error uploading to S3: {str(e)}")
        raise


def get_s3_file_url(s3_key: str) -> str:
    """
    Generate S3 file URL
    """
    return f"s3://{S3_BUCKET_NAME}/{s3_key}"


async def process_pdf(s3_file_url: str, s3_key: str, document_id: str):
    try:
        instances = _get_global_instances()
        pdf = instances['pdf']
        
        pdf.set_path(s3_file_url)
        pdf.set_bucket_name(S3_BUCKET_NAME)
        pdf.set_key(s3_key)
        sentences = pdf.read_chunks()

        await _add_data_to_qdrant(sentences, document_id)
        await _add_data_to_neo4j(sentences, document_id)

        print("üéâ PDF processing ho√†n t·∫•t.")
    except Exception as e:
        print(f"‚ùå Error during processing: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error {str(e)}")


# Th√™m file m·ªõi v√†o qdrant
async def _add_data_to_qdrant(sentences, document_id):
    print('in qdrant: ' + document_id)
    try:
        instances = _get_global_instances()
        llama_chunks = instances['llama_chunks']
        qdrant = instances['qdrant']

        chunks = []
        all_paragraphs = []

        llama_chunks.set_prompt(chunking())

        for idx, s in enumerate(sentences):
            print("vƒÉn b·∫£n ban ƒë·∫ßu: " + s)
            llama_chunks.set_text(s)
            print(f"\n--- Sentence {idx + 1}/{len(sentences)} ---")
            try:
                raw_output = llama_chunks.generator()
                print("[L·∫ßn 1] Raw output:", repr(raw_output))

                # N·∫øu mu·ªën parse Python literal
                chunk_json = ast.literal_eval(raw_output)
                print("[L·∫ßn 1] Parsed data:", chunk_json)

            except Exception as e1:
                print("[L·∫ßn 1] L·ªói parse:", e1)
                traceback.print_exc()

                try:
                    raw_output_2 = llama_chunks.generator()
                    print("[L·∫ßn 2] Raw output:", repr(raw_output_2))

                    chunk_json = ast.literal_eval(raw_output_2)
                    print("[L·∫ßn 2] Parsed data:", chunk_json)

                except Exception as e2:
                    print("[L·∫ßn 2] L·ªói parse:", e2)
                    traceback.print_exc()
                    raise HTTPException(
                        status_code=500,
                        detail=f"Kh√¥ng parse ƒë∆∞·ª£c chunk_json cho c√¢u {idx + 1}"
                    )

            chunks.append(chunk_json)

        # T·∫°o m·∫£ng c√°c paragraph
        for chunk in chunks:
            for key, content in chunk.items():
                all_paragraphs.append(content)

        print("\nT·ªïng s·ªë paragraph:", len(all_paragraphs))
        print("Danh s√°ch paragraph:", all_paragraphs)

        # 1. t·∫°o collection trong qdrant
        qdrant.set_collection_name(document_id)
        qdrant.create_collection()

        # 2. T·∫°o embedding
        embeddings_dict = qdrant.create_embed(all_paragraphs)
        print("Embeddings ƒë√£ t·∫°o, s·ªë l∆∞·ª£ng:", len(embeddings_dict))

        # 3. l∆∞u v√†o qdrant
        qdrant.add_data(all_paragraphs, embeddings_dict)
        print("ƒê√£ l∆∞u v√†o Qdrant th√†nh c√¥ng.")

        return {
            "message": "File uploaded and processed successfully for qdrant",
            "data": document_id
        }

    except Exception as e:
        print(f"[ERROR] Error processing file in qdrant: {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Error processing file in qdrant: {str(e)}"
        )


async def _add_data_to_neo4j(sentences, document_id):
    try:
        print("\n[DEBUG] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file cho Neo4j")
        instances = _get_global_instances()
        llama_title = instances['llama_title']
        llama_content = instances['llama_content']
        neo = instances['neo']
        pre_processing = instances['pre_processing']

        # =========================
        # B∆Ø·ªöC 1: T·∫†O TI√äU ƒê·ªÄ
        # =========================
        titles = []
        llama_title.set_prompt(create_title())
        print("\n[DEBUG] T·∫°o ti√™u ƒë·ªÅ cho t·ª´ng c√¢u...")
        for i, s in enumerate(sentences):
            try:
                llama_title.set_text(s)
                raw_title = llama_title.generator().lower()
                print(f"\n[Title {i + 1}] Raw output:", repr(raw_title))

                title_json = pre_processing.string_to_json(raw_title)
                print(f"[Title {i + 1}] Parsed JSON:", title_json)

                titles.append(title_json)

                # Sleep ƒë·ªÉ tr√°nh rate-limit n·∫øu c·∫ßn
                if i % 2 == 0 and i != 0:
                    print(f"[DEBUG] Sleep 45s sau c√¢u th·ª© {i + 1}...")
                    time.sleep(45)

            except Exception as e:
                print(f"[ERROR] L·ªói khi x·ª≠ l√Ω ti√™u ƒë·ªÅ c√¢u {i + 1}: {e}")
                traceback.print_exc()
                raise HTTPException(status_code=500, detail=f"L·ªói parse ti√™u ƒë·ªÅ t·∫°i c√¢u {i + 1}: {e}")

        print("\n[DEBUG] Danh s√°ch ti√™u ƒë·ªÅ cu·ªëi c√πng:", titles)

        # =========================
        # B∆Ø·ªöC 2: T·∫†O ENTITIES & RELATIONSHIPS
        # =========================
        entities_relationship = []
        llama_content.set_prompt(extract_entities_relationship_from_text())
        print("\n[DEBUG] T·∫°o entities & relationships cho t·ª´ng c√¢u...")

        for i, s in enumerate(sentences):
            try:
                llama_content.set_text(s)
                raw_er = llama_content.generator().lower()
                print(f"\n[Entities {i + 1}] Raw output:", repr(raw_er))

                match = re.search(r'(\{\s*"relationships".*\})', raw_er, re.DOTALL)
                if match:
                    json_str = match.group(1)
                    try:
                        er_json = json.loads(json_str)
                        print(f"[Entities {i + 1}] Parsed JSON:", er_json)
                        entities_relationship.append(er_json)
                    except json.JSONDecodeError as e:
                        try:
                            raw_er = llama_content.generator().lower()
                            match = re.search(r'(\{\s*"relationships".*\})', raw_er, re.DOTALL)
                            if match:
                                json_str = match.group(1)
                                er_json = json.loads(json_str)
                                print(f"[Entities {i + 1}] Parsed JSON:", er_json)
                                entities_relationship.append(er_json)
                        except:
                            print("L·ªói parse JSON:", e)
            except Exception as e:
                print(f"[ERROR] L·ªói khi x·ª≠ l√Ω entities c√¢u {i + 1}: {e}")
                traceback.print_exc()
                raise HTTPException(status_code=500, detail=f"L·ªói parse entities t·∫°i c√¢u {i + 1}: {e}")

        print("\n[DEBUG] Danh s√°ch entities_relationship cu·ªëi c√πng:", entities_relationship)

        # =========================
        # B∆Ø·ªöC 3: L∆ØU V√ÄO NEO4J
        # =========================
        print("\n[DEBUG] Th√™m node & relationship v√†o Neo4j...")

        try:
            # B1: N·ªëi "General" v·ªõi Document
            neo.add_single_relationship("t√†i li·ªáu", "General", document_id, "Document", "BAO_G·ªíM")
            print(f"[Neo4j] ƒê√£ n·ªëi 'General' -> Document {document_id}")

            # B2: N·ªëi Document v·ªõi ti√™u ƒë·ªÅ
            for title in titles:
                neo.add_single_relationship(document_id, "Document", title["title"], "Part", "BAO_G·ªíM")
                print(f"[Neo4j] ƒê√£ n·ªëi Document {document_id} -> Part {title['title']}")

            # B3: N·ªëi ti√™u ƒë·ªÅ v·ªõi entities_relationship
            for r, title in zip(entities_relationship, titles):
                neo.import_relationships(r, title["title"], "Part")
                print(f"[Neo4j] ƒê√£ import relationships cho Part {title['title']}")

        except Exception as e:
            print(f"[ERROR] L·ªói khi insert v√†o Neo4j: {e}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"L·ªói insert v√†o Neo4j: {e}")

        print("\n[DEBUG] Ho√†n t·∫•t th√™m d·ªØ li·ªáu v√†o Neo4j.")
        return {
            "message": "File uploaded and processed successfully for neo4j",
            "data": document_id
        }

    except Exception as e:
        print(f"[ERROR] Error processing file in neo4j: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error processing file in neo4j: {str(e)}")


def create_contact_message(db: Session, contact: ContactMessageCreate):
    """Create a new contact message"""
    db_contact = ContactMessage(
        name=contact.name,
        email=contact.email,
        subject=contact.subject,
        message=contact.message
    )
    
    db.add(db_contact)
    db.commit()
    db.refresh(db_contact)
    
    return db_contact